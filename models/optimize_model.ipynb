{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import bernoulli\n",
    "from bitstring import BitArray\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140,) (916,) (2140,) (916,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BORE_OIL_VOL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATEPRD</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-02-12</th>\n",
       "      <td>0.030164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-13</th>\n",
       "      <td>0.198133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-14</th>\n",
       "      <td>0.331061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-15</th>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-16</th>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BORE_OIL_VOL\n",
       "DATEPRD                 \n",
       "2008-02-12      0.030164\n",
       "2008-02-13      0.198133\n",
       "2008-02-14      0.331061\n",
       "2008-02-15      0.276400\n",
       "2008-02-16      0.323400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/consolidated_data/oil_data.csv\", index_col=\"DATEPRD\")\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled, index=df.index, columns=df.columns)\n",
    "\n",
    "date = df.index\n",
    "scaled_oil_data = scaled_df.BORE_OIL_VOL.to_numpy()\n",
    "\n",
    "split_date = 2140\n",
    "\n",
    "date_train = date[:split_date]\n",
    "date_test = date[split_date:]\n",
    "oil_train = scaled_oil_data[:split_date]\n",
    "oil_test = scaled_oil_data[split_date:]\n",
    "\n",
    "print(oil_train.shape, oil_test.shape, date_train.shape, date_test.shape)\n",
    "\n",
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh():\n",
    "    keras.backend.clear_session()\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "class ResetStatesCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "def sequential_window_dataset(series, window_size):\n",
    "    \n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=window_size, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[1:]))\n",
    "    \n",
    "    return dataset.batch(1).prefetch(1)\n",
    "\n",
    "# def prepare_dataset(data, window_size):\n",
    "#     X, Y = np.empty((0,window_size)), np.empty((0))\n",
    "#     for i in range(len(data)-window_size-1):\n",
    "#         X = np.vstack([X,data[i:(i + window_size),0]])\n",
    "#         Y = np.append(Y,data[i + window_size,0])   \n",
    "#     X = np.reshape(X,(len(X),window_size,1))\n",
    "#     Y = np.reshape(Y,(len(Y),1))\n",
    "#     return X, Y\n",
    "\n",
    "def train_model(train_dataset, validation_dataset, num_units):\n",
    "    model = Sequential([\n",
    "                    LSTM(num_units, return_sequences=True, stateful=True, batch_input_shape=[1, None, 1]),\n",
    "                    LSTM(num_units, return_sequences=True, stateful=True),\n",
    "                    Dense(1)\n",
    "            ])\n",
    "    optimizer = keras.optimizers.SGD(\n",
    "                    learning_rate=1e-3, \n",
    "                    momentum=0.9\n",
    "                )\n",
    "    model.compile(\n",
    "                    loss=keras.losses.Huber(), \n",
    "                    optimizer=optimizer, \n",
    "                    metrics=[\"mae\"]\n",
    "                )\n",
    "    reset_states = ResetStatesCallback()\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "                    \"LSTM_oil_checkpoint.h5\", \n",
    "                    save_best_only=True\n",
    "                    )\n",
    "    early_stopping = EarlyStopping(patience=50)\n",
    "    model.fit(\n",
    "                    train_dataset, \n",
    "                    epochs=500, \n",
    "                    verbose=0,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[\n",
    "                                early_stopping, \n",
    "                                model_checkpoint, \n",
    "                                reset_states\n",
    "                            ]\n",
    "            )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_evaluate(ga_individual_solution):   \n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    window_size_bits = BitArray(ga_individual_solution[0:8])\n",
    "    num_units_bits = BitArray(ga_individual_solution[8:]) \n",
    "    window_size = window_size_bits.uint\n",
    "    num_units = num_units_bits.uint\n",
    "    print('\\nWindow Size: ', window_size, ', Num of Units: ', num_units)\n",
    "    \n",
    "    # Return fitness score of 100 if window_size or num_unit is zero\n",
    "    if window_size == 0 or num_units == 0:\n",
    "        return 100, \n",
    "    \n",
    "    # Segment the train_data based on new window_size; split into train and validation (80/20)\n",
    "    refresh()\n",
    "\n",
    "    oil_train_set = sequential_window_dataset(oil_train, window_size)\n",
    "    oil_test_set = sequential_window_dataset(oil_test, window_size)\n",
    "    \n",
    "    # Train LSTM model and predict on validation set\n",
    "    model = train_model(oil_train_set, oil_test_set, num_units)\n",
    "    \n",
    "    #load best model\n",
    "    model = keras.models.load_model(\"LSTM_oil_checkpoint.h5\")\n",
    "    LSTM_oil_forecast = model.predict(scaled_oil_data[np.newaxis, :, np.newaxis])\n",
    "    LSTM_oil_forecast = LSTM_oil_forecast[0, split_date - 1:-1, 0]\n",
    "\n",
    "    # # Calculate the RMSE score as fitness score for GA\n",
    "    mae = keras.metrics.mean_absolute_error(oil_test, LSTM_oil_forecast).numpy()\n",
    "    mse = keras.metrics.mean_squared_error(oil_test, LSTM_oil_forecast).numpy()\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    print(f'mae = {mae}, \\nmse = {mse}, \\nrmse = {rmse}')\n",
    "    \n",
    "    return rmse,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = 4\n",
    "num_generations = 4\n",
    "gene_length = 16\n",
    "\n",
    "# As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "# In case, when you want to maximize accuracy for instance, use 1.0\n",
    "creator.create('FitnessMax', base.Fitness, weights = (-1.0,))\n",
    "creator.create('Individual', list , fitness = creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('binary', bernoulli.rvs, 0.5)\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\n",
    "toolbox.register('population', tools.initRepeat, list , toolbox.individual)\n",
    "\n",
    "toolbox.register('mate', tools.cxOrdered)\n",
    "toolbox.register('mutate', tools.mutShuffleIndexes, indpb = 0.6)\n",
    "toolbox.register('select', tools.selRoulette)\n",
    "toolbox.register('evaluate', train_evaluate)\n",
    "\n",
    "population = toolbox.population(n = population_size)\n",
    "r = algorithms.eaSimple(population, toolbox, cxpb = 0.4, mutpb = 0.1, ngen = num_generations, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individuals = tools.selBest(population,k = 1)\n",
    "best_window_size = None\n",
    "best_num_units = None\n",
    "\n",
    "for bi in best_individuals:\n",
    "    window_size_bits = BitArray(bi[0:8])\n",
    "    num_units_bits = BitArray(bi[8:]) \n",
    "    best_window_size = window_size_bits.uint\n",
    "    best_num_units = num_units_bits.uint\n",
    "    print('\\nWindow Size: ', best_window_size, ', Num of Units: ', best_num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = prepare_dataset(train_data,best_window_size)\n",
    "X_test, y_test = prepare_dataset(test_data,best_window_size)\n",
    "\n",
    "inputs = Input(shape=(best_window_size,1))\n",
    "x = LSTM(best_num_units, input_shape=(best_window_size,1))(inputs)\n",
    "predictions = Dense(1, activation='linear')(x)\n",
    "model = Model(inputs = inputs, outputs = predictions)\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=10,shuffle=True)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print('Test RMSE: ', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "133444414d1ee3d5e92372fd828872450c43ed97795e9b0f6d2c2bdc325c8146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
